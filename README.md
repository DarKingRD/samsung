# Умный корректор текста (аналог Grammarly)

Проект: веб-приложение для исправления ошибок в тексте с подсветкой ошибок и несколькими вариантами исправления.

Обучение и эксперименты проводились на Kaggle: https://www.kaggle.com/code/darkingrd/rut5-base-train

## Возможности

- Исправление текста и выдача нескольких вариантов (top-n).
- Подсветка ошибок (возвращается HTML для отображения на фронтенде).
- Типы ошибок: `punctuation`, `grammar`, `spelling`, `semantics`.
- Веб-интерфейс (Flask + JS).

## Структура проекта

```
├── data
│   ├── processed               # Обработанные датасеты
│       └── ...               
│   └── raw                     # Необработанные датасеты
│       └── ... 
├── models
│   └── ...                     # Здесь должна располагаться модель
│
├── src/
    ├── model.py                # Модель
    ├── train.py                # Файл для локального обучения модели
│   ├── app.py                  # Flask приложение + API + Swagger (Flasgger)
│   ├── inference.py            # Инференс модели и логика исправлений/подсветки
│   └── data_processing.py      # Подготовка/обработка данных (скрипты)
├── web/
│   ├── openapi.yaml            # OpenAPI спецификация для Swagger UI
│   ├── templates/
│   │   └── index.html          # Главная страница
│   └── static/
│       ├── css/main.css        # Стили
│       └── js/main.js          # Логика фронтенда (fetch к API)
└── README.md                   # Описание проекта (офигеть!)
```

## Установка

1) Создать окружение и поставить зависимости:

```bash
pip install -r requirements.txt
```

2) Подготовить папку с моделью:

- По умолчанию приложение ищет модель в `./models/correction_model_v2`.
  - Модель можно скачать с нашего kaggle: [ссылка на модель](https://www.kaggle.com/code/darkingrd/rut5-base-train)
  - В папке `./models/correction_model_v2` должно быть две папки с файлами: `models` и `tokenizer`. Также, если есть файл `config.json`, то загрузить его нужно следующим образом: `./models/correction_model_v2/config.json`
  > Почему correction_model_v2? Да просто немного впадлу менять в коде загрузку модели, ну и при разработке использовалось куча версий данной модели. v2 получилось самой лучшей)
- Если папки нет — будет использована базовая `t5-small`.

3) Загрузить датасеты в папку `data/raw/`

- датасет kartaslov загрузить в папку data/raw/kartaslov (там два файла `orfo_and_typos.L1_5.csv` и `orfo_and_typos.L1_5+PHON.csv`, ссылочка на github этих файлов: [Kartaslov orfo_and_typos](https://github.com/dkulagin/kartaslov/tree/master/dataset/orfo_and_typos))
- датасет LORuGEC загрузить в папку data/raw/loru (скачать нужно `LORuGEC.xlsx` файл ссылочка на данный датасет: [LORuGEC](https://github.com/ReginaNasyrova/LORuGEC))
## Запуск

Запуск веб-приложения:

```bash
python src/app.py
```

После запуска:

- Веб-интерфейс: `http://localhost:5000/`
- Swagger UI: `http://localhost:5000/apidocs/` (через Flasgger)

## API

### POST /api/correct

Исправляет текст и возвращает несколько вариантов.

Пример запроса:

```json
{ "text": "Я хочю ета зделать." }
```

### POST /api/highlight

Возвращает HTML с подсветкой ошибок и количество ошибок для выбранного варианта (`variant_index`).

Пример запроса:

```json
{ "text": "Я хочю ета зделать.", "variant_index": 0 }
```

### GET /api/stats

Простой endpoint для проверки статуса приложения (загружена ли модель и версия).

## Датасеты

Используемые источники данных:

- LORuGEC.
- Kartaslov.
- Синтетика на основе предыдущих источников.


## Текущее положение дел

В настоящий момент, как уже можно понаблюдать по датасетам, есть проблема в количестве пунтуационных, грамматических и смысловых ошибок. На данный момент у нас недостаточно данных, из-за чего наша модель исправляет хорошо орфографические ошибки, а остальные уже как получится. Поэтому следующие наши шаги:
- найти новые датасеты с грамматическими, пунтуационными и смысловыми ошибками
- достичь балансировки датасета, чтобы типы ошибок были в соотношении 1:1:1:1 
(в идеале)

---

Работа выполнена студентами 4‑го курса направления 01.03.02 "Прикладная математика и информатика" Северного (Арктического) федерального университета имени М. В. Ломоносова — Денисом Барановым и Ярославом Дунаевым — в рамках курса "Искусственный интеллект" от Samsung Innovation Campus.