import pandas as pd
import numpy as np
import warnings
from pathlib import Path
import re
import random
from typing import List, Tuple, Dict
from tqdm import tqdm

warnings.filterwarnings('ignore')

# ============================================================================
# –°–¢–†–£–ö–¢–£–†–ê –ö–ê–¢–ê–õ–û–ì–û–í
# ============================================================================
BASE_DIR = Path('.')
RAW_DIR = BASE_DIR / "data" / "raw"
PROCESSED_DIR = BASE_DIR / "data" / "processed"

PROCESSED_DIR.mkdir(parents=True, exist_ok=True)

# ============================================================================
# –§–£–ù–ö–¶–ò–ò –û–ß–ò–°–¢–ö–ò
# ============================================================================

def is_russian_text(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç —Ä—É—Å—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã"""
    russian_chars = re.findall(r'[–∞-—è—ë–ê-–Ø–Å]', str(text))
    return len(russian_chars) > 0

def looks_like_code(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ—Ö–æ–∂ –ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞ –∫–æ–¥"""
    if not isinstance(text, str):
        return False
    
    code_patterns = [
        r'<[^>]+>',
        r'\$\{[^}]+\}',
        r'function|const|let|var|=>',
        r'def |class |import ',
        r'[(){}\[\]{}]',
        r'^\s*//',
        r'^\s*\*',
    ]
    
    return any(re.search(pattern, text, re.IGNORECASE) for pattern in code_patterns)

def looks_like_markup(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ—Ö–æ–∂ –ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä–∞–∑–º–µ—Ç–∫—É"""
    if not isinstance(text, str):
        return False
    
    markup_patterns = [
        r'<[^>]+>',
        r'^\s*#+\s+',
        r'^\s*[-*]\s+',
        r'\[.+\]\(.+\)',
        r'```|~~~',
        r'``.+``',
    ]
    
    return any(re.search(pattern, text, re.MULTILINE | re.IGNORECASE) 
               for pattern in markup_patterns)

def is_valid_text(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≤–∞–ª–∏–¥–Ω—ã–π –ª–∏ —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    text = str(text).strip()
    
    if not text:
        return False
    if looks_like_code(text) or looks_like_markup(text):
        return False
    if len(text) < 3:
        return False
    if not is_russian_text(text):
        return False
    
    return True

# ============================================================================
# –°–ò–ù–¢–ï–¢–ò–ß–ï–°–ö–ò–ï –î–ê–ù–ù–´–ï - –í–°–ï –¢–ò–ü–´ –û–®–ò–ë–û–ö
# ============================================================================

class SyntheticErrorGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫ –î–õ–Ø –í–°–ï–• –¢–ò–ü–û–í"""
    
    # –ü–æ—Ö–æ–∂–∏–µ —Ä—É—Å—Å–∫–∏–µ –±—É–∫–≤—ã
    SIMILAR_CHARS = {
        '–æ': '–∞',
        '–∞': '–æ',
        '–µ': '–∏',
        '–∏': '–µ',
        '—Å': '—Ü',
        '—Ü': '—Å',
        '—à': '—â',
        '—â': '—à',
        '–ª': '–Ω',
        '–Ω': '–ª',
        '—Ä': '–ø',
        '–ø': '—Ä',
        '–±': '–≤',
        '–≤': '–±',
        '–¥': '—Ç',
        '—Ç': '–¥',
        '—Ö': '–∫',
        '–∫': '—Ö',
        '–∂': '–∑',
        '–∑': '–∂',
        '–≥': '–∫',
        '–º': '–ª',
    }
    
    # ===== –û–†–§–û–ì–†–ê–§–ò–ß–ï–°–ö–ò–ï –û–®–ò–ë–ö–ò =====
    
    @staticmethod
    def typo_swap_chars(word: str) -> str:
        """–ó–∞–º–µ–Ω–∞ —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ (—Ç—Ä–∞–Ω—Å–ø–æ–∑–∏—Ü–∏—è)"""
        if len(word) < 2:
            return word
        pos = random.randint(0, len(word) - 2)
        word_list = list(word)
        word_list[pos], word_list[pos + 1] = word_list[pos + 1], word_list[pos]
        return ''.join(word_list)
    
    @staticmethod
    def typo_delete_char(word: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª–∞ (–ø—Ä–æ–ø—É—Å–∫ –±—É–∫–≤—ã)"""
        if len(word) < 2:
            return word
        pos = random.randint(0, len(word) - 1)
        return word[:pos] + word[pos + 1:]
    
    @staticmethod
    def typo_duplicate_char(word: str) -> str:
        """–î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–º–≤–æ–ª–∞"""
        if len(word) < 1:
            return word
        pos = random.randint(0, len(word) - 1)
        return word[:pos + 1] + word[pos] + word[pos + 1:]
    
    @staticmethod
    def typo_replace_similar(word: str) -> str:
        """–ó–∞–º–µ–Ω–∞ –Ω–∞ –ø–æ—Ö–æ–∂–∏–π —Å–∏–º–≤–æ–ª"""
        for char, similar in SyntheticErrorGenerator.SIMILAR_CHARS.items():
            if char in word:
                pos = word.index(char)
                return word[:pos] + similar + word[pos + 1:]
        return word
    
    @staticmethod
    def typo_insert_char(word: str) -> str:
        """–í—Å—Ç–∞–≤–∫–∞ —Å–∏–º–≤–æ–ª–∞"""
        if len(word) < 1:
            return word
        russian_alphabet = '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è'
        pos = random.randint(0, len(word))
        char = random.choice(russian_alphabet)
        return word[:pos] + char + word[pos:]
    
    @staticmethod
    def typo_replace_char(word: str) -> str:
        """–ó–∞–º–µ–Ω–∞ –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã–π —Å–∏–º–≤–æ–ª"""
        if len(word) < 1:
            return word
        russian_alphabet = '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è'
        pos = random.randint(0, len(word) - 1)
        char = random.choice(russian_alphabet)
        return word[:pos] + char + word[pos + 1:]
    
    # ===== –ü–£–ù–ö–¢–£–ê–¶–ò–û–ù–ù–´–ï –û–®–ò–ë–ö–ò =====
    
    @staticmethod
    def punctuation_remove_comma(text: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø—è—Ç–æ–π"""
        if ',' not in text:
            return text
        pos = text.index(',')
        return text[:pos] + text[pos + 1:]
    
    @staticmethod
    def punctuation_add_comma(text: str) -> str:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–∏—à–Ω–µ–π –∑–∞–ø—è—Ç–æ–π"""
        words = text.split()
        if len(words) < 2:
            return text
        pos = random.randint(0, len(words) - 2)
        words[pos] = words[pos] + ','
        return ' '.join(words)
    
    @staticmethod
    def punctuation_period_to_comma(text: str) -> str:
        """–ó–∞–º–µ–Ω–∞ —Ç–æ—á–∫–∏ –Ω–∞ –∑–∞–ø—è—Ç—É—é"""
        if '.' not in text:
            return text
        return text.replace('.', ',', 1)
    
    @staticmethod
    def punctuation_remove_period(text: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ —Ç–æ—á–∫–∏ –≤ –∫–æ–Ω—Ü–µ"""
        if text.endswith('.'):
            return text[:-1]
        return text
    
    # ===== –ì–†–ê–ú–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –û–®–ò–ë–ö–ò =====
    
    @staticmethod
    def grammar_ne_agreement(text: str) -> str:
        """–û—à–∏–±–∫–∞ –≤ –Ω–∞–ø–∏—Å–∞–Ω–∏–∏ "–Ω–µ" —Å —Å–ª–æ–≤–∞–º–∏"""
        words = text.split()
        
        # –ü—Ä–∏–º–µ—Ä—ã: –Ω–µ —Å–º–µ–ª–æ—Å—Ç—å -> –Ω–µ—Å–º–µ–ª–æ—Å—Ç—å
        ne_words = {
            '–Ω–µ —Å–º–µ–ª–æ—Å—Ç—å': '–Ω–µ—Å–º–µ–ª–æ—Å—Ç—å',
            '–Ω–µ –∑—Ä–µ–ª–æ—Å—Ç—å': '–Ω–µ–∑—Ä–µ–ª–æ—Å—Ç—å',
            '–Ω–µ –¥–æ–≤–µ—Ä–∏–µ': '–Ω–µ–¥–æ–≤–µ—Ä–∏–µ',
            '–Ω–µ –≤–Ω–∏–º–∞–Ω–∏–µ': '–Ω–µ–≤–Ω–∏–º–∞–Ω–∏–µ',
            '–Ω–µ –æ–±—Ö–æ–¥': '–Ω–µ–æ–±—Ö–æ–¥',
            '–Ω–µ –Ω—É–∂–Ω–æ': '–Ω–µ–Ω—É–∂–Ω–æ',
            '–Ω–µ –¥–æ–ª–∂–Ω–æ': '–Ω–µ–¥–æ–ª–∂–Ω–æ',
        }
        
        for i in range(len(words) - 1):
            phrase = (words[i] + ' ' + words[i + 1]).lower()
            if phrase in ne_words:
                # –ú–µ–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ: –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π -> –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π
                words[i] = ne_words[phrase]
                words[i + 1] = ''
                return ' '.join(w for w in words if w)
        
        return text
    
    @staticmethod
    def grammar_case_error(text: str) -> str:
        """–û—à–∏–±–∫–∞ –≤ –ø–∞–¥–µ–∂–µ (—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ)"""
        # –ü—Ä–∏–º–µ—Ä—ã
        replacements = [
            ('—Å –æ—Ç–ø—É—Å–∫–∞', '–∏–∑ –æ—Ç–ø—É—Å–∫–∞'),
            ('–ø–æ –ø—Ä–æ—à–µ—Å—Ç–≤–∏—é', '–ø–æ –ø—Ä–æ—à–µ—Å—Ç–≤–∏–∏'),
            ('–¥–æ –Ω–∞—Å', '–Ω–∞—Å'),
            ('–≤ –¥–µ–ª–µ', '–Ω–∞ –¥–µ–ª–µ'),
        ]
        
        for incorrect, correct in replacements:
            if incorrect in text:
                return text.replace(incorrect, correct, 1)
        
        return text
    
    @staticmethod
    def grammar_tense_error(text: str) -> str:
        """–û—à–∏–±–∫–∞ –≤ –≤—Ä–µ–º–µ–Ω–∏ –≥–ª–∞–≥–æ–ª–∞"""
        replacements = [
            ('–±—ã–ª', '–±—ã–ª–æ'),
            ('–±—ã–ª–∞', '–±—ã–ª–æ'),
            ('–∏–º–µ—é', '–∏–º–µ—é'),
            ('–¥–µ–ª–∞—é', '–¥–µ–ª–∞–ª'),
        ]
        
        for old, new in replacements:
            if old in text:
                return text.replace(old, new, 1)
        
        return text
    
    # ===== –°–ú–´–°–õ–û–í–´–ï –û–®–ò–ë–ö–ò =====
    
    @staticmethod
    def semantic_word_order(text: str) -> str:
        """–û—à–∏–±–∫–∞ –≤ –ø–æ—Ä—è–¥–∫–µ —Å–ª–æ–≤"""
        words = text.split()
        
        if len(words) >= 3:
            # –°–ª—É—á–∞–π–Ω–æ –ø–µ—Ä–µ—Å—Ç–∞–≤–ª—è–µ–º –¥–≤–∞ —Å–ª–æ–≤–∞
            i = random.randint(0, len(words) - 2)
            words[i], words[i + 1] = words[i + 1], words[i]
        
        return ' '.join(words)
    
    @staticmethod
    def semantic_wrong_word(text: str) -> str:
        """–ó–∞–º–µ–Ω–∞ —Å–ª–æ–≤–∞ –Ω–∞ –ø–æ—Ö–æ–∂–µ–µ –ø–æ —Å–º—ã—Å–ª—É –Ω–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ"""
        replacements = [
            ('–±–ª–∞–≥–æ–¥–∞—Ä—è', '–∏–∑-–∑–∞'),
            ('–æ–¥–Ω–∞–∫–æ', '–ø–æ—Ç–æ–º—É —á—Ç–æ'),
            ('—Ö–æ—Ç—è', '—Ç–∞–∫ –∫–∞–∫'),
            ('–ø–æ—Ç–æ–º—É —á—Ç–æ', '–Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞'),
        ]
        
        for old, new in replacements:
            if old in text:
                return text.replace(old, new, 1)
        
        return text
    
    # ===== –°–¢–ò–õ–ò–°–¢–ò–ß–ï–°–ö–ò–ï –û–®–ò–ë–ö–ò =====
    
    @staticmethod
    def stylistic_repetition(text: str) -> str:
        """–ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —Å–ª–æ–≤–∞"""
        words = text.split()
        
        if len(words) >= 2:
            # –ü–æ–≤—Ç–æ—Ä—è–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ —Å–ª–æ–≤–æ
            i = random.randint(0, len(words) - 1)
            words.insert(i + 1, words[i])
        
        return ' '.join(words)
    
    @staticmethod
    def stylistic_formal_to_informal(text: str) -> str:
        """–°–º–µ—à–∏–≤–∞–Ω–∏–µ —Å—Ç–∏–ª–µ–π: —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–µ -> –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–æ–µ"""
        replacements = [
            ('–≤—ã—Å–æ–∫–æ—É–≤–∞–∂–∞–µ–º—ã–π', '–ø—Ä–∏–≤–µ—Ç'),
            ('–ø–æ–∑–≤–æ–ª—å—Ç–µ', '–¥–∞–≤–∞–π—Ç–µ'),
            ('—Å–æ–¥–µ–π—Å—Ç–≤–∏–µ', '–ø–æ–º–æ—â—å'),
            ('–ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–µ', '–ø—Ä–æ–±–ª–µ–º–∞'),
        ]
        
        for old, new in replacements:
            if old in text:
                return text.replace(old, new, 1)
        
        return text
    
    # ===== –ì–ï–ù–ï–†–ê–¢–û–† =====
    
    @classmethod
    def generate(cls, text: str, error_type: str = None) -> Tuple[str, str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—à–∏–±–∫—É –≤ —Ç–µ–∫—Å—Ç–µ
        
        Returns:
            (error_text, error_category)
        """
        if len(text) < 3:
            return text, 'none'
        
        generators = [
            # –û—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ
            (cls.typo_swap_chars, 'spelling'),
            (cls.typo_delete_char, 'spelling'),
            (cls.typo_duplicate_char, 'spelling'),
            (cls.typo_replace_similar, 'spelling'),
            (cls.typo_insert_char, 'spelling'),
            (cls.typo_replace_char, 'spelling'),
            
            # –ü—É–Ω–∫—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–µ
            (cls.punctuation_remove_comma, 'punctuation'),
            (cls.punctuation_add_comma, 'punctuation'),
            (cls.punctuation_period_to_comma, 'punctuation'),
            (cls.punctuation_remove_period, 'punctuation'),
            
            # –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ
            (cls.grammar_ne_agreement, 'grammar'),
            (cls.grammar_case_error, 'grammar'),
            (cls.grammar_tense_error, 'grammar'),
            
            # –°–º—ã—Å–ª–æ–≤—ã–µ
            (cls.semantic_word_order, 'semantics'),
            (cls.semantic_wrong_word, 'semantics'),
            
            # –°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ
            (cls.stylistic_repetition, 'stylistic'),
            (cls.stylistic_formal_to_informal, 'stylistic'),
        ]
        
        if error_type:
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–∏–ø—É
            generators = [g for g in generators if g[1] == error_type]
            if not generators:
                return text, 'none'
        
        generator, error_cat = random.choice(generators)
        
        try:
            # –î–ª—è –º–µ—Ç–æ–¥–æ–≤ —Å–æ —Å–ª–æ–≤–∞–º–∏ –Ω—É–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∫ –æ—Ç–¥–µ–ª—å–Ω–æ–º—É —Å–ª–æ–≤—É
            if error_cat == 'spelling' and hasattr(generator, '__name__') and 'word' in str(text).lower():
                words = text.split()
                if words:
                    idx = random.randint(0, len(words) - 1)
                    words[idx] = generator(words[idx])
                    return ' '.join(words), error_cat
            
            result = generator(text)
            if result != text:
                return result, error_cat
            else:
                return text, 'none'
        except:
            return text, 'none'

def generate_synthetic_dataset(source_texts: List[str], 
                              num_per_text: int = 3,
                              error_types: List[str] = None) -> pd.DataFrame:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –∏–∑ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
    
    Args:
        source_texts: –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        num_per_text: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –Ω–∞ —Ç–µ–∫—Å—Ç
        error_types: —Ç–∏–ø—ã –æ—à–∏–±–æ–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    """
    if error_types is None:
        error_types = ['spelling', 'punctuation', 'grammar', 'semantics', 'stylistic']
    
    records = []
    
    for text in tqdm(source_texts, desc="ü§ñ –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ", leave=False):
        for error_type in error_types[:num_per_text]:
            try:
                error_text, actual_type = SyntheticErrorGenerator.generate(text, error_type)
                
                if error_text != text and is_valid_text(error_text):
                    records.append({
                        'source': error_text,
                        'target': text,
                        'weight': 1.0,
                        'type': 'synthetic',
                        'error_category': actual_type
                    })
            except:
                continue
    
    return pd.DataFrame(records)

# ============================================================================
# –û–ë–†–ê–ë–û–¢–ß–ò–ö–ò –î–ê–ù–ù–´–•
# ============================================================================

def process_kartaslov() -> pd.DataFrame:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –ö–ê–†–¢–ê–°–õ–û–í–ê
    –¢–∏–ø: –û–†–§–û–ì–†–ê–§–ò–ß–ï–°–ö–ò–ï –û–®–ò–ë–ö–ò (–æ–ø–µ—á–∞—Ç–∫–∏)
    """
    dfs = []
    
    files = [
        RAW_DIR / "kartaslov" / "orfo_and_typos.L1_5.csv",
        RAW_DIR / "kartaslov" / "orfo_and_typos.L1_5-PHON.csv",
        Path("orfo_and_typos.L1_5.csv"),
        Path("orfo_and_typos.L1_5-PHON.csv"),
    ]
    
    for csv_file in files:
        if not csv_file.exists():
            continue
        
        try:
            df = pd.read_csv(csv_file, sep=';', on_bad_lines='skip')
            
            if len(df) == 0:
                continue
            
            print(f"üìÑ {csv_file.name}")
            
            cols = {col.upper(): col for col in df.columns}
            correct_col = cols.get('CORRECT')
            mistake_col = cols.get('MISTAKE')
            weight_col = cols.get('WEIGHT')
            
            if not correct_col or not mistake_col:
                continue
            
            df_clean = pd.DataFrame({
                'source': df[mistake_col].astype(str).str.strip(),
                'target': df[correct_col].astype(str).str.strip(),
                'weight': df[weight_col].astype(float) if weight_col else 1.0,
                'type': 'kartaslov',
                'error_category': 'spelling'
            })
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            before = len(df_clean)
            df_clean = df_clean[
                (df_clean['source'] != df_clean['target']) &
                (df_clean['source'].str.len() >= 2) &
                (df_clean['target'].str.len() >= 2) &
                (df_clean['source'].apply(is_russian_text)) &
                (df_clean['target'].apply(is_russian_text))
            ].reset_index(drop=True)
            after = len(df_clean)
            
            print(f"   ‚úì {after:,} –ø—Ä–∏–º–µ—Ä–æ–≤ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫")
            dfs.append(df_clean)
            
        except Exception as e:
            print(f"   ‚úó –û—à–∏–±–∫–∞: {e}")
    
    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

def process_lorugec() -> Dict[str, pd.DataFrame]:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ LORUGEC
    –¢–∏–ø—ã: –ì–†–ê–ú–ú–ê–¢–ò–ß–ï–°–ö–ò–ï, –ü–£–ù–ö–¢–£–ê–¶–ò–û–ù–ù–´–ï, –°–ú–´–°–õ–û–í–´–ï –æ—à–∏–±–∫–∏
    
    Returns:
        dict —Å –æ—à–∏–±–∫–∞–º–∏ –ø–æ —Ç–∏–ø–∞–º: grammar, punctuation, semantics
    """
    
    files = [
        RAW_DIR / "loru" / "LORuGEC.xlsx",
        Path("LORuGEC.xlsx"),
    ]
    
    for xlsx_file in files:
        if not xlsx_file.exists():
            continue
        
        try:
            df = pd.read_excel(xlsx_file, sheet_name=0)
            
            print(f"üìÑ {xlsx_file.name}")
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
            initial_col = next((col for col in df.columns if 'Initial' in col), None)
            correct_col = next((col for col in df.columns if 'Correct' in col and 'Initial' not in col), None)
            section_col = next((col for col in df.columns if 'section' in col.lower()), None)
            
            if not initial_col or not correct_col or not section_col:
                print(f"   ‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω—ã –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã")
                continue
            
            # –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫
            type_map = {
                'Spelling': 'spelling',
                'Punctuation': 'punctuation',
                'Grammar': 'grammar',
                'Semantics': 'semantics',
            }
            
            result = {}
            
            for section, error_cat in type_map.items():
                section_df = df[df[section_col] == section].copy()
                
                if len(section_df) == 0:
                    continue
                
                df_clean = pd.DataFrame({
                    'source': section_df[initial_col].astype(str).str.strip(),
                    'target': section_df[correct_col].astype(str).str.strip(),
                    'weight': 1.0,
                    'type': 'lorugec',
                    'error_category': error_cat
                })
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
                before = len(df_clean)
                df_clean = df_clean[
                    (df_clean['source'] != df_clean['target']) &
                    (df_clean['source'].str.len() >= 3) &
                    (df_clean['target'].str.len() >= 3) &
                    (df_clean['source'].apply(is_valid_text)) &
                    (df_clean['target'].apply(is_valid_text))
                ].reset_index(drop=True)
                after = len(df_clean)
                
                if after > 0:
                    print(f"   ‚úì {after:,} –ø—Ä–∏–º–µ—Ä–æ–≤ {error_cat}")
                    result[error_cat] = df_clean
            
            return result
            
        except Exception as e:
            print(f"   ‚úó –û—à–∏–±–∫–∞: {e}")
    
    return {}

# ============================================================================
# –û–°–ù–û–í–ù–û–ô –ü–†–û–¶–ï–°–°
# ============================================================================

print("\n" + "="*80)
print("üöÄ –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –°–ò–°–¢–ï–ú–´ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –û–®–ò–ë–û–ö")
print("="*80 + "\n")

print("–ó–∞–¥–∞—á–∞: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–ø–µ—á–∞—Ç–æ–∫ –í–°–ï–• —Ç–∏–ø–æ–≤")
print("  ‚Ä¢ –û—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ (–æ–ø–µ—á–∞—Ç–∫–∏, –∑–∞–º–µ–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∏)")
print("  ‚Ä¢ –ü—É–Ω–∫—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–µ (–∑–∞–ø—è—Ç—ã–µ, —Ç–æ—á–∫–∏)")
print("  ‚Ä¢ –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ (—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ, –ø–∞–¥–µ–∂–∏, –≤—Ä–µ–º–µ–Ω–∞)")
print("  ‚Ä¢ –°–º—ã—Å–ª–æ–≤—ã–µ (–ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤, –≤—ã–±–æ—Ä —Å–ª–æ–≤–∞)")
print("  ‚Ä¢ –°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ (–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è, —Å—Ç–∏–ª—å)\n")

print("="*80)
print("üìã –®–ê–ì 1: –ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï")
print("="*80 + "\n")

# –ö–∞—Ä—Ç–∞—Ålov - –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ
print("1Ô∏è‚É£  –ö–ê–†–¢–ê–°LOV (–æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏)")
print("-" * 80)
kartaslov_df = process_kartaslov()

# LORuGEC - –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ, –ø—É–Ω–∫—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–µ, —Å–º—ã—Å–ª–æ–≤—ã–µ
print("\n2Ô∏è‚É£  LORUGEC (–≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ, –ø—É–Ω–∫—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–µ, —Å–º—ã—Å–ª–æ–≤—ã–µ)")
print("-" * 80)
lorugec_dict = process_lorugec()

# –®–ê–ì 2: –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ
print("\n" + "="*80)
print("üîó –®–ê–ì 2: –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –ò–°–•–û–î–ù–´–• –î–ê–ù–ù–´–•")
print("="*80 + "\n")

all_original = []

if len(kartaslov_df) > 0:
    kartaslov_df.to_csv(PROCESSED_DIR / "kartaslov_spelling.csv", index=False)
    all_original.append(kartaslov_df)
    print(f"‚úÖ kartaslov_spelling.csv: {len(kartaslov_df):,} –ø—Ä–∏–º–µ—Ä–æ–≤")

for error_cat, df in lorugec_dict.items():
    if len(df) > 0:
        filename = f"lorugec_{error_cat}.csv"
        df.to_csv(PROCESSED_DIR / filename, index=False)
        all_original.append(df)
        print(f"‚úÖ {filename}: {len(df):,} –ø—Ä–∏–º–µ—Ä–æ–≤")

if all_original:
    original_combined = pd.concat(all_original, ignore_index=True)
    original_combined = original_combined.drop_duplicates(subset=['source', 'target'])
    print(f"\n‚úÖ –í—Å–µ–≥–æ –∏—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(original_combined):,}")
else:
    original_combined = pd.DataFrame()
    print("‚ùå –ù–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

# –®–ê–ì 3: –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
print("\n" + "="*80)
print("ü§ñ –®–ê–ì 3: –°–ò–ù–¢–ï–¢–ò–ß–ï–°–ö–ò–ï –î–ê–ù–ù–´–ï (–¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫)")
print("="*80 + "\n")

if len(original_combined) > 0:
    # –ë–µ—Ä–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫
    correct_texts = original_combined['target'].unique().tolist()
    
    print(f"–ì–µ–Ω–µ—Ä–∏—Ä—É—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –∏–∑ {len(correct_texts):,} —Ç–µ–∫—Å—Ç–æ–≤...")
    print("  ‚Ä¢ –û—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ (swap, delete, duplicate, replace)")
    print("  ‚Ä¢ –ü—É–Ω–∫—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–µ (–∑–∞–ø—è—Ç—ã–µ, —Ç–æ—á–∫–∏)")
    print("  ‚Ä¢ –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ (—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ, —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ)")
    print("  ‚Ä¢ –°–º—ã—Å–ª–æ–≤—ã–µ (–ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤, –≤—ã–±–æ—Ä —Å–ª–æ–≤–∞)")
    print("  ‚Ä¢ –°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ (–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è, —Å—Ç–∏–ª—å)\n")
    
    synthetic_df = generate_synthetic_dataset(
        correct_texts,
        num_per_text=5,  # 5 —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫ –Ω–∞ –∫–∞–∂–¥—ã–π —Ç–µ–∫—Å—Ç
        error_types=['spelling', 'punctuation', 'grammar', 'semantics', 'stylistic']
    )
    
    if len(synthetic_df) > 0:
        synthetic_df.to_csv(PROCESSED_DIR / "synthetic_errors.csv", index=False)
        print(f"\n‚úÖ synthetic_errors.csv: {len(synthetic_df):,} –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        # –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        print("\n–ü–æ —Ç–∏–ø–∞–º –æ—à–∏–±–æ–∫:")
        for cat, count in synthetic_df['error_category'].value_counts().items():
            print(f"  ‚Ä¢ {cat}: {count:,}")
    else:
        synthetic_df = pd.DataFrame()
        print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ")
else:
    synthetic_df = pd.DataFrame()

# –®–ê–ì 4: –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
print("\n" + "="*80)
print("‚ú® –®–ê–ì 4: –§–ò–ù–ê–õ–¨–ù–´–ô –î–ê–¢–ê–°–ï–¢")
print("="*80 + "\n")

all_data = []

if len(original_combined) > 0:
    all_data.append(original_combined)

if len(synthetic_df) > 0:
    all_data.append(synthetic_df)

if all_data:
    final_df = pd.concat(all_data, ignore_index=True)
    final_df = final_df.drop_duplicates(subset=['source', 'target'])
    final_df = final_df.reset_index(drop=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    final_df.to_csv(PROCESSED_DIR / "all_train.csv", index=False)
    print(f"‚úÖ all_train.csv: {len(final_df):,} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ì–æ—Ç–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    train_df = final_df[['source', 'target', 'error_category', 'weight']].copy()
    train_df.columns = ['input_text', 'output_text', 'error_type', 'weight']
    train_df.to_csv(PROCESSED_DIR / "all_train_enhanced.csv", index=False)
    print(f"‚úÖ all_train_enhanced.csv: {len(train_df):,} –ø—Ä–∏–º–µ—Ä–æ–≤ ‚Üê –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø")
else:
    final_df = pd.DataFrame()
    print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö")

# ============================================================================
# –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò –ü–†–ò–ú–ï–†–´
# ============================================================================

print("\n" + "="*80)
print("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
print("="*80 + "\n")

if len(final_df) > 0:
    print(f"üìà –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(final_df):,}\n")
    
    # –ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    print("–ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
    for src, count in final_df['type'].value_counts().items():
        print(f"  ‚Ä¢ {src}: {count:,}")
    
    # –ü–æ —Ç–∏–ø–∞–º –æ—à–∏–±–æ–∫
    print("\n–ü–æ —Ç–∏–ø–∞–º –æ—à–∏–±–æ–∫:")
    for error_type, count in final_df['error_category'].value_counts().items():
        print(f"  ‚Ä¢ {error_type}: {count:,}")
    
    # –ü—Ä–∏–º–µ—Ä—ã
    print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã (–ø–æ —Ç–∏–ø–∞–º –æ—à–∏–±–æ–∫):")
    
    for error_type in ['spelling', 'punctuation', 'grammar', 'semantics', 'stylistic']:
        sample = final_df[final_df['error_category'] == error_type].head(1)
        if len(sample) > 0:
            row = sample.iloc[0]
            print(f"\n  üîπ {error_type.upper()}")
            print(f"     ‚ùå {row['source'][:70]}")
            print(f"     ‚úÖ {row['target'][:70]}")

print("\n" + "="*80)
print("‚úÖ –ì–û–¢–û–í–û!")
print("="*80)
print("\nüìÇ –í—ã—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ data/processed/:")
print("   ‚úÖ kartaslov_spelling.csv - –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ")
print("   ‚úÖ lorugec_*.csv - –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ, –ø—É–Ω–∫—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–µ, —Å–º—ã—Å–ª–æ–≤—ã–µ")
print("   ‚úÖ synthetic_errors.csv - —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ (–≤—Å–µ —Ç–∏–ø—ã)")
print("   ‚úÖ all_train.csv - –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç")
print("   ‚úÖ all_train_enhanced.csv - –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
print("\nüöÄ –î–∞–ª—å—à–µ: python improved_train.py\n")
