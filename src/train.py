"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫.
"""

import torch
from torch.utils.data import Dataset
import pandas as pd
from pathlib import Path
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
)
import json
import time
import os

DATA_DIR = Path("data/processed")
MODELS_DIR = Path("models")
MODELS_DIR.mkdir(exist_ok=True)

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –∫—ç—à—É
def get_cache_dir():
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—É—Ç—å –∫ –∫—ç—à—É Hugging Face."""
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—É—Ç–∏ –∫—ç—à–∞
    cache_paths = [
        Path.home() / ".cache" / "huggingface" / "hub",
        Path(os.environ.get("HF_HOME", "")) / "hub",
        MODELS_DIR / "cache",
    ]
    
    for path in cache_paths:
        if path.exists():
            print(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à: {path}")
            return str(path)
    
    # –ï—Å–ª–∏ –∫—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –≤ –ø–∞–ø–∫–µ models
    cache_path = MODELS_DIR / "cache"
    cache_path.mkdir(exist_ok=True)
    print(f"üìÅ –°–æ–∑–¥–∞–µ–º –∫—ç—à –≤: {cache_path}")
    return str(cache_path)

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—É—Ç—å –∫ –∫—ç—à—É
CACHE_DIR = get_cache_dir()
os.environ['TRANSFORMERS_CACHE'] = CACHE_DIR
os.environ['HF_HOME'] = str(MODELS_DIR)

print(f"\n‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏:")
print(f"   –ö—ç—à –º–æ–¥–µ–ª–µ–π: {CACHE_DIR}")
print(f"   –ü–∞–ø–∫–∞ –º–æ–¥–µ–ª–µ–π: {MODELS_DIR}")
print(f"   –î–∞–Ω–Ω—ã–µ: {DATA_DIR}")

class TypoDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫."""
    
    def __init__(self, csv_path: Path, tokenizer, max_length: int = 128):
        self.data = pd.read_csv(csv_path)
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        original = str(self.data.iloc[idx]['original'])
        corrected = str(self.data.iloc[idx]['corrected'])
        
        # –î–ª—è T5 –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –∑–∞–¥–∞—á–∏
        original = "–∏—Å–ø—Ä–∞–≤—å –æ–ø–µ—á–∞—Ç–∫—É: " + original
        
        inputs = self.tokenizer(
            original,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ü–µ–ª–µ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–±–µ–∑ deprecated as_target_tokenizer)
        targets = self.tokenizer(
            text_target=[corrected],
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # –ó–∞–º–µ–Ω—è–µ–º pad_token_id –Ω–∞ -100 –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ loss
        labels = targets['input_ids'].squeeze()
        labels[labels == self.tokenizer.pad_token_id] = -100
        
        return {
            'input_ids': inputs['input_ids'].squeeze(),
            'attention_mask': inputs['attention_mask'].squeeze(),
            'labels': labels,
        }


def load_model_simple(model_name: str = "ai-forever/ruT5-base"):
    """–ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∫—ç—à–∞."""
    print(f"\n{'='*60}")
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
    print(f"{'='*60}")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=CACHE_DIR,
            legacy=False  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        )
        print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            cache_dir=CACHE_DIR
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        print(f"\nüìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏:")
        print(f"   –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {model.config.model_type}")
        print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(tokenizer)}")
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {'GPU' if torch.cuda.is_available() else 'CPU'}")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print("1. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –∑–∞–ø—É—Å—Ç–∏–ª–∏ preload_model.py")
        print("2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
        print("3. –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –≤ –∫—ç—à–µ, –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –ø—É—Ç—å –Ω–∞–ø—Ä—è–º—É—é:")
        print(f"   model_name = '{CACHE_DIR}/models--ai-forever--ruT5-base'")
        raise


def train_seq2seq_model(csv_path: Path):
    """–û–±—É—á–∞–µ—Ç seq2seq –º–æ–¥–µ–ª—å (T5) –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫."""
    print(f"\nüìä –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {csv_path}")
    
    if not csv_path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª {csv_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = pd.read_csv(csv_path)
    print(f"üìà –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤
    print("\nüëÄ –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
    for i in range(min(3, len(df))):
        print(f"   {i+1}. '{df.iloc[i]['original']}' -> '{df.iloc[i]['corrected']}'")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    model, tokenizer = load_model_simple()
    
    # –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç
    print("\nüìö –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç...")
    dataset = TypoDataset(csv_path, tokenizer, max_length=128)
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val
    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    print(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {train_size} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö, {val_size} –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è T5)
    training_args = TrainingArguments(
        output_dir=str(MODELS_DIR / "checkpoints"),
        num_train_epochs=3,
        per_device_train_batch_size=4 if torch.cuda.is_available() else 2,  # –£–º–µ–Ω—å—à–∏–ª –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        per_device_eval_batch_size=4,
        learning_rate=3e-4,
        warmup_steps=100,
        weight_decay=0.01,
        logging_dir=str(MODELS_DIR / "logs"),
        logging_steps=20,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        fp16=torch.cuda.is_available(),
        dataloader_pin_memory=torch.cuda.is_available(),
        gradient_accumulation_steps=2,
        report_to="none",
        dataloader_num_workers=0,  # 0 –¥–ª—è Windows, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º
        remove_unused_columns=False,  # –í–∞–∂–Ω–æ –¥–ª—è T5
    )
    
    # Data collator
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True
    )
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("\n" + "="*60)
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    print(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {'GPU' if torch.cuda.is_available() else 'CPU'}")
    print(f"üìà –≠–ø–æ—Ö: {training_args.num_train_epochs}")
    print(f"üì¶ Batch size: {training_args.per_device_train_batch_size}")
    print("="*60 + "\n")
    
    try:
        trainer.train()
        print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return None
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        raise
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_path = MODELS_DIR / "typo_corrector_model"
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ {model_path}...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    model.save_pretrained(model_path)
    tokenizer.save_pretrained(model_path)
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = {
        "model_type": "ruT5-base",
        "trained_on": time.strftime("%Y-%m-%d %H:%M:%S"),
        "dataset_size": len(df),
        "max_length": 128,
        "task_prefix": "–∏—Å–ø—Ä–∞–≤—å –æ–ø–µ—á–∞—Ç–∫—É: "
    }
    
    with open(model_path / "config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    return model_path


def train_simple_model(csv_path: Path):
    """–£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∞–≤–∏–ª–∞—Ö."""
    print(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {csv_path}")
    df = pd.read_csv(csv_path)
    
    # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å –æ–ø–µ—á–∞—Ç–æ–∫
    typo_dict = {}
    for _, row in df.iterrows():
        original = str(row['original']).strip().lower()
        corrected = str(row['corrected']).strip().lower()
        if original != corrected:
            typo_dict[original] = corrected
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å
    dict_path = MODELS_DIR / "typo_dict.json"
    with open(dict_path, 'w', encoding='utf-8') as f:
        json.dump(typo_dict, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Å–ª–æ–≤–∞—Ä—å –∏–∑ {len(typo_dict)} –ø–∞—Ä –æ–ø–µ—á–∞—Ç–æ–∫")
    print(f"üìÅ –°–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {dict_path}")
    
    return dict_path


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è."""
    csv_path = DATA_DIR / "typo_corpus.csv"
    
    if not csv_path.exists():
        print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª {csv_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ src/data_processing.py")
        return
    
    print("\n" + "=" * 60)
    print("ü§ñ –í–´–ë–û–† –ú–ï–¢–û–î–ê –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 60)
    print("1. Seq2Seq –º–æ–¥–µ–ª—å (T5)")
    print("2. –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∞–≤–∏–ª–∞—Ö")
    print("=" * 60)
    
    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä
    try:
        choice = input("\nüéØ –í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (1 –∏–ª–∏ 2): ").strip()
        if choice not in ["1", "2"]:
            print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç 2 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
            choice = "2"
    except:
        choice = "2"
    
    print(f"\n‚úÖ –í—ã–±—Ä–∞–Ω –º–µ—Ç–æ–¥: {'Seq2Seq –º–æ–¥–µ–ª—å' if choice == '1' else '–£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å'}")
    print("-" * 60)
    
    if choice == "1":
        try:
            train_seq2seq_model(csv_path)
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
            print("\nüîÑ –ü—Ä–æ–±—É–µ–º —É–ø—Ä–æ—â—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
            train_simple_model(csv_path)
    else:
        train_simple_model(csv_path)
    
    print("\n" + "=" * 60)
    print("üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 60)


if __name__ == "__main__":
    main()