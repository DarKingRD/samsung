"""
train.py - –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Trainer,
    TrainingArguments,
)
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
import logging
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# –î–ê–¢–ê–°–ï–¢
# ============================================================================


class TextCorrectionDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤"""

    def __init__(self, csv_path: str, tokenizer, max_length: int = 128):
        """
        Args:
            csv_path: –ø—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É
            tokenizer: tokenizer –∏–∑ transformers
            max_length: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
        """
        self.max_length = max_length
        self.tokenizer = tokenizer

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self.data = pd.read_csv(csv_path)

        # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ
        self.data = self.data.dropna(subset=["input_text", "output_text"])
        self.data = self.data[
            (self.data["input_text"].str.len() > 0)
            & (self.data["output_text"].str.len() > 0)
        ]

        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {csv_path}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]

        input_text = "fix: " + str(row['input_text'])

        # –ö–æ–¥–∏—Ä—É–µ–º input
        input_encoding = self.tokenizer(
            input_text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        # –ö–æ–¥–∏—Ä—É–µ–º output (target)
        output_encoding = self.tokenizer(
            output_text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        return {
            "input_ids": input_encoding["input_ids"].squeeze(),
            "attention_mask": input_encoding["attention_mask"].squeeze(),
            "labels": output_encoding["input_ids"].squeeze(),
        }


# ============================================================================
# –û–ë–£–ß–ï–ù–ò–ï
# ============================================================================


class TextCorrectionTrainer:
    """Trainer –¥–ª—è –º–æ–¥–µ–ª–∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤"""

    def __init__(self, model_name: str = "t5-small", device: str = "cuda"):
        """
        Args:
            model_name: –º–æ–¥–µ–ª—å –∏–∑ HuggingFace
            device: cuda –∏–ª–∏ cpu
        """
        self.device = device
        self.model_name = model_name

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ tokenizer
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é {model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        self.model.to(device)

        logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    def train(
        self,
        train_csv: str,
        output_dir: str = "./models/correction_model",
        num_epochs: int = 3,
        batch_size: int = 8,
        learning_rate: float = 5e-5,
        validation_split: float = 0.1,
    ):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å

        Args:
            train_csv: –ø—É—Ç—å –∫ CSV —Å –¥–∞–Ω–Ω—ã–º–∏
            output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
            num_epochs: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            learning_rate: learning rate
            validation_split: –¥–æ–ª—è validation –¥–∞–Ω–Ω—ã—Ö
        """

        logger.info("=" * 80)
        logger.info("üöÄ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ö–û–†–†–ï–ö–¶–ò–ò –¢–ï–ö–°–¢–ê")
        logger.info("=" * 80)

        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        logger.info("\nüìã –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        dataset = TextCorrectionDataset(train_csv, self.tokenizer)

        # Split –Ω–∞ train/val
        train_size = int(len(dataset) * (1 - validation_split))
        val_size = len(dataset) - train_size

        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )

        logger.info(f"   Train: {len(train_dataset)}")
        logger.info(f"   Val: {len(val_dataset)}")

        # DataLoaders
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size)

        # Optimizer
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)

        # Training loop
        logger.info("\nüìñ –û–ë–£–ß–ï–ù–ò–ï:")
        logger.info("-" * 80)

        self.model.train()
        total_steps = num_epochs * len(train_loader)
        current_step = 0

        for epoch in range(num_epochs):
            logger.info(f"\n–≠–ø–æ—Ö–∞ {epoch + 1}/{num_epochs}")

            epoch_loss = 0

            # Training
            progress_bar = tqdm(train_loader, desc="Training")
            for batch in progress_bar:
                optimizer.zero_grad()

                input_ids = batch["input_ids"].to(self.device)
                attention_mask = batch["attention_mask"].to(self.device)
                labels = batch["labels"].to(self.device)

                # Forward pass
                outputs = self.model(
                    input_ids=input_ids, attention_mask=attention_mask, labels=labels
                )

                loss = outputs.loss
                epoch_loss += loss.item()

                # Backward pass
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()

                current_step += 1
                progress_bar.set_postfix({"loss": loss.item()})

            avg_loss = epoch_loss / len(train_loader)
            logger.info(f"   Train Loss: {avg_loss:.4f}")

            # Validation
            self.model.eval()
            val_loss = 0
            with torch.no_grad():
                for batch in val_loader:
                    input_ids = batch["input_ids"].to(self.device)
                    attention_mask = batch["attention_mask"].to(self.device)
                    labels = batch["labels"].to(self.device)

                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels,
                    )

                    val_loss += outputs.loss.item()

            avg_val_loss = val_loss / len(val_loader)
            logger.info(f"   Val Loss: {avg_val_loss:.4f}")

            self.model.train()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        logger.info("\n" + "=" * 80)
        logger.info("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
        logger.info("=" * 80)

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ tokenizer
        self.model.save_pretrained(output_path / "model")
        self.tokenizer.save_pretrained(output_path / "tokenizer")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        config = {
            "model_name": self.model_name,
            "max_length": 128,
            "learning_rate": learning_rate,
            "batch_size": batch_size,
            "num_epochs": num_epochs,
        }

        with open(output_path / "config.json", "w") as f:
            json.dump(config, f, indent=2)

        logger.info(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_path}")
        logger.info(f"   üìÅ Model: {output_path / 'model'}")
        logger.info(f"   üìÅ Tokenizer: {output_path / 'tokenizer'}")
        logger.info(f"   üìÑ Config: {output_path / 'config.json'}")

        return self.model


# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞")
    parser.add_argument("--model", type=str, default="cointegrated/rut5-base", help="–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å")
    parser.add_argument(
        "--data",
        type=str,
        default="data/processed/all_train_enhanced.csv",
        help="CSV —Å –¥–∞–Ω–Ω—ã–º–∏",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="./models/correction_model",
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è",
    )
    parser.add_argument("--epochs", type=int, default=3, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö")
    parser.add_argument("--batch", type=int, default=8, help="Batch size")
    parser.add_argument("--lr", type=float, default=5e-5, help="Learning rate")
    parser.add_argument("--device", type=str, default="cuda", help="cuda –∏–ª–∏ cpu")

    args = parser.parse_args()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
    if not Path(args.data).exists():
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Ä—Ç–∞—Å–ª–æ–≤—Å–∫–∏–π —Ñ–∞–π–ª –µ—Å–ª–∏ –Ω–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if Path("orfo_and_typos.L1_5.csv").exists():
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–∞—Ä—Ç–∞—Å–ª–æ–≤—Å–∫–∏–π —Ñ–∞–π–ª
            logger.info("–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é –∫–∞—Ä—Ç–∞—Å–ª–æ–≤—Å–∫–∏–π —Ñ–∞–π–ª...")
            df = pd.read_csv("orfo_and_typos.L1_5.csv", sep=";", on_bad_lines="skip")

            # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã
            df.columns = ["input_text", "output_text", "weight"]

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            Path("data/processed").mkdir(parents=True, exist_ok=True)
            df.to_csv("data/processed/all_train_enhanced.csv", index=False)
            args.data = "data/processed/all_train_enhanced.csv"
            logger.info(f"‚úÖ –§–∞–π–ª –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω: {args.data}")
        else:
            logger.error(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {args.data}")
            exit(1)

    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    trainer = TextCorrectionTrainer(model_name=args.model, device=args.device)
    trainer.train(
        train_csv=args.data,
        output_dir=args.output,
        num_epochs=args.epochs,
        batch_size=args.batch,
        learning_rate=args.lr,
    )

    print("\n" + "=" * 80)
    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 80)
    print(f"\nüöÄ –î–∞–ª—å—à–µ:")
    print(f"   python inference.py --model {args.output}")
    print(f"   python app.py --model {args.output}")
